{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Feature extractor network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"Domain critic network.\"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class WDGRL():\n",
    "    def __init__(self, input_dim: int=2, generator_hidden_dims: List[int]=[32, 16, 8, 4, 2], critic_hidden_dims: List[int]=[32, 16, 8, 4, 2],\n",
    "                 gamma: float = 0.1, _lr_generator: float = 1e-2, _lr_critic: float = 1e-2, \n",
    "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.generator = Generator(input_dim, generator_hidden_dims).to(self.device)\n",
    "        self.critic = Critic(generator_hidden_dims[-1], critic_hidden_dims).to(self.device)\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=_lr_generator)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=_lr_critic)\n",
    "    \n",
    "    # def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "    #     \"\"\"Compute gradient penalty.\"\"\"\n",
    "    #     if source_data.size(0) > target_data.size(0):\n",
    "    #         ms = source_data.size(0)\n",
    "    #         mt = target_data.size(0)\n",
    "    #         gradient_penalty = 0\n",
    "    #         for _ in range(0, ms, mt):\n",
    "    #             source_chunk = source_data[_:_+mt]\n",
    "    #             target_chunk = target_data\n",
    "    #             alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "    #             interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "    #             # Domain critic outputs\n",
    "    #             dc_output = self.critic(interpolates)\n",
    "                \n",
    "    #             # Compute gradients\n",
    "    #             gradients = torch.autograd.grad(\n",
    "    #                 outputs=dc_output,\n",
    "    #                 inputs=interpolates,\n",
    "    #                 grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "    #                 create_graph=True,\n",
    "    #                 retain_graph=True,\n",
    "    #                 only_inputs=True,\n",
    "    #             )\n",
    "    #             gradients = gradients[0]\n",
    "    #             gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    #         if ms % mt != 0:\n",
    "    #             source_chunk = source_data[ms-mt:]\n",
    "    #             perm = torch.randperm(mt)\n",
    "    #             idx = perm[:ms % mt]\n",
    "    #             target_chunk = target_data[idx]\n",
    "    #             alpha = torch.rand(target_chunk.size(0), 1).to(self.device)\n",
    "    #             interpolates = (alpha * source_chunk + ((1 - alpha) * target_chunk)).requires_grad_(True)\n",
    "                \n",
    "    #             # Domain critic outputs\n",
    "    #             dc_output = self.critic(interpolates)\n",
    "                \n",
    "    #             # Compute gradients\n",
    "    #             gradients = torch.autograd.grad(\n",
    "    #                 outputs=dc_output,\n",
    "    #                 inputs=interpolates,\n",
    "    #                 grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "    #                 create_graph=True,\n",
    "    #                 retain_graph=True,\n",
    "    #                 only_inputs=True,\n",
    "    #             )\n",
    "    #             gradients = gradients[0]\n",
    "    #             gradient_penalty += ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    #         return gradient_penalty / ((ms // mt) + (ms % mt != 0)) \n",
    "        \n",
    "    #     # For balanced batch\n",
    "    #     alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "    #     interpolates = (alpha * source_data + ((1 - alpha) * target_data)).requires_grad_(True)\n",
    "        \n",
    "    #     # Domain critic outputs\n",
    "    #     dc_output = self.critic(interpolates)\n",
    "        \n",
    "    #     # Compute gradients\n",
    "    #     gradients = torch.autograd.grad(\n",
    "    #         outputs=dc_output,\n",
    "    #         inputs=interpolates,\n",
    "    #         grad_outputs=torch.ones_like(dc_output).to(self.device),\n",
    "    #         create_graph=True,\n",
    "    #         retain_graph=True,\n",
    "    #         only_inputs=True,\n",
    "    #     )[0]\n",
    "\n",
    "    #     # Compute gradient penalty\n",
    "    #     gradients = gradients.view(gradients.size(0), -1)\n",
    "    #     return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    def compute_gradient_penalty(self, source_data: torch.Tensor, target_data: torch.Tensor) -> torch.Tensor:\n",
    "        alpha = torch.rand(source_data.size(0), 1).to(self.device)\n",
    "        differences = target_data - source_data \n",
    "        interpolates = source_data + (alpha * differences)\n",
    "        interpolates = torch.stack([interpolates, source_data, target_data]).requires_grad_()\n",
    "\n",
    "        preds = self.critic(interpolates)\n",
    "        gradients = torch.autograd.grad(preds, interpolates,\n",
    "                     grad_outputs=torch.ones_like(preds),\n",
    "                     retain_graph=True, create_graph=True)[0]\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        gradient_penalty = ((gradient_norm - 1)**2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "\n",
    "    def train(self, source_loader: DataLoader, target_loader: DataLoader, num_epochs: int = 100, dc_iter: int = 100) -> List[float]:\n",
    "        self.generator.train()\n",
    "        self.critic.train()\n",
    "        losses = []\n",
    "        source_critic_scores = []\n",
    "        target_critic_scores = []\n",
    "        for epoch in trange(num_epochs, desc='Epoch'):\n",
    "            loss = 0\n",
    "            total_loss = 0\n",
    "            for (source_data, _), (target_data, _) in zip(source_loader, target_loader):\n",
    "                source_data, target_data = source_data.to(self.device), target_data.to(self.device)\n",
    "\n",
    "                # Train domain critic\n",
    "                for _ in range(dc_iter):\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        source_features = self.generator(source_data).view(source_data.size(0), -1)\n",
    "                        target_features = self.generator(target_data).view(target_data.size(0), -1)\n",
    "                    \n",
    "                    # Compute empirical Wasserstein distance\n",
    "                    dc_source = self.critic(source_features)\n",
    "                    dc_target = self.critic(target_features)\n",
    "                    wasserstein_distance = (dc_source.mean() - dc_target.mean())\n",
    "\n",
    "                    # Gradient penalty\n",
    "                    gradient_penalty = self.compute_gradient_penalty(source_features, target_features)\n",
    "\n",
    "                    # Domain critic loss\n",
    "                    dc_loss = - wasserstein_distance + self.gamma * gradient_penalty\n",
    "                    print(f'- iteration #{_} / {dc_iter} | source critic: {dc_source.mean().item()} | target critic: {dc_target.mean().item()} | wasserstein distance: {wasserstein_distance.item()} | gradient penalty: {gradient_penalty.item()}')\n",
    "                    dc_loss.backward()\n",
    "                    self.critic_optimizer.step()\n",
    "                    with torch.no_grad():\n",
    "                        total_loss += wasserstein_distance.item()\n",
    "                print('-------------------------------')\n",
    "                # Train feature extractor\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                source_features = self.generator(source_data)\n",
    "                target_features = self.generator(target_data)\n",
    "                dc_source = self.critic(source_features)\n",
    "                dc_target = self.critic(target_features)\n",
    "                wasserstein_distance = (dc_source.mean() - dc_target.mean())\n",
    "                wasserstein_distance.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    loss += wasserstein_distance.item()\n",
    "                \n",
    "                    \n",
    "            source_critic_scores.append(self.criticize(source_loader.dataset.tensors[0].to(self.device)))\n",
    "            target_critic_scores.append(self.criticize(target_loader.dataset.tensors[0].to(self.device)))\n",
    "            losses.append(loss/len(source_loader))\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {loss/len(source_loader)}')\n",
    "            print('--------------------------------')\n",
    "        return losses, source_critic_scores, target_critic_scores\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.generator.eval()\n",
    "        return self.generator(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def criticize(self, x: torch.Tensor) -> float:\n",
    "        self.generator.eval()\n",
    "        self.critic.eval()\n",
    "        return self.critic(self.generator(x)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "def gen_data(mu: float, delta: List[int], n: int, d: int):\n",
    "    mu = np.full((n, d), mu, dtype=np.float64)\n",
    "    noise = np.random.normal(loc = 0, scale = 1, size=(n, d))\n",
    "    X = mu + noise\n",
    "    labels = np.zeros(n)\n",
    "    # 5% of the data is abnormal.\n",
    "    # Anomalies are generated by randomly adding deltas to the data.\n",
    "    n_anomalies = int(n * 0.05)\n",
    "    idx = np.random.choice(n, n_anomalies, replace=False)\n",
    "    if 0 in delta: \n",
    "        delta.pop(delta.index(0))\n",
    "    split_points = sorted(np.random.choice(range(1, len(idx)), len(delta) - 1, replace=False))\n",
    "    segments = np.split(idx, split_points)\n",
    "    for i, segment in enumerate(segments):\n",
    "        X[segment] = X[segment] + delta[i]\n",
    "    labels[idx] = 1\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Create synthetic dataset and dataloaders for domain adaptation.\"\"\"\n",
    "# Create datasets\n",
    "ns, nt, d = 1000, 100, 1\n",
    "\n",
    "xs, ys = gen_data(0, [0, 1, 2, 3, 4], ns, d)\n",
    "xt, yt = gen_data(2, [0, 1, 2, 3, 4], nt, d)\n",
    "\n",
    "plt.scatter(xs[:, 0], np.zeros_like(xs[:, 0]), c=ys, cmap='viridis', s=2)\n",
    "plt.scatter(xt[:, 0], np.zeros_like(xt[:, 0]), c=yt, cmap='cool', s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_divisors(N_A, N_B):\n",
    "    gcd = np.gcd(N_A, N_B)  # Step 1: Compute GCD\n",
    "    return [d for d in range(1, gcd + 1) if gcd % d == 0]\n",
    "\n",
    "common_divisors(ns, nt)\n",
    "cd = [d for d in reversed(common_divisors(ns, nt)) if nt//d >= 5]\n",
    "cd = cd[0]\n",
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "xs = torch.FloatTensor(xs)\n",
    "ys = torch.LongTensor(ys)\n",
    "xt = torch.FloatTensor(xt)\n",
    "yt = torch.LongTensor(yt)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "source_dataset = TensorDataset(xs, ys)\n",
    "target_dataset = TensorDataset(xt, yt)\n",
    "source_loader = DataLoader(source_dataset, batch_size=cd, shuffle=True)\n",
    "target_loader = DataLoader(target_dataset, batch_size=cd, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_hidden_dims = [10, 10, 10]\n",
    "critic_hidden_dims = [4, 2, 1]\n",
    "model = WDGRL(input_dim=d, generator_hidden_dims=generator_hidden_dims, critic_hidden_dims=critic_hidden_dims)\n",
    "num_epochs = 5\n",
    "losses, source_critic_scores, target_critic_scores = model.train(source_loader, target_loader, num_epochs=num_epochs, dc_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_hat = model.extract_feature(xs.cuda())\n",
    "xt_hat = model.extract_feature(xt.cuda())\n",
    "xs_hat = xs_hat.cpu().numpy()\n",
    "xt_hat = xt_hat.cpu().numpy()\n",
    "print(xs_hat)\n",
    "print(xt_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(xs_hat, axis=1))\n",
    "print(np.sum(xt_hat, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(np.sum(xs_hat, axis=1)))\n",
    "print(max(np.sum(xt_hat, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), losses, 'b-', label='Domain Critic Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Domain Critic Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), source_critic_scores, 'b-', label='Source Critic Score')\n",
    "plt.plot(range(1, num_epochs+1), target_critic_scores, 'r-', label='Target Critic Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Critic Score (Source vs Target)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with all the necessary components\n",
    "checkpoint = {\n",
    "    'generator_state_dict': model.generator.state_dict(),\n",
    "    'critic_state_dict': model.critic.state_dict(),\n",
    "    'device': model.device,\n",
    "}\n",
    "index = None\n",
    "\n",
    "with open(\"models.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        words = line[:-1].split(\"/\")\n",
    "        if words[1] == str(generator_hidden_dims) and words[2] == str(critic_hidden_dims):\n",
    "            index = i\n",
    "            break\n",
    "if index is None:\n",
    "    with open(\"models.txt\", \"r\") as f:\n",
    "        index = len(f.readlines())\n",
    "    with open(\"models.txt\", \"a\") as f:\n",
    "        f.write(f\"{index}/{generator_hidden_dims}/{critic_hidden_dims}\\n\")\n",
    "# Save the checkpoint\n",
    "torch.save(checkpoint, f\"wdgrl_{index}.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
